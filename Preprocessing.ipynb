{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e75f2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Human filled label table\n",
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"zh_core_web_lg\")\n",
    "data_path = \"data/question_label.csv\"\n",
    "\n",
    "ac = pd.read_csv(data_path,sep=\",\",header=0)\n",
    "\n",
    "count = 0\n",
    "total = 100\n",
    "# using dependency as chunk_tags\n",
    "with open(\"chunk_question.csv\", 'w') as f:\n",
    "    f.write(\"text,chunk_tags,pos_tags,ner_tags\\n\")\n",
    "    for index, row in ac.iterrows():\n",
    "        # Line Note\n",
    "        if count == 0:\n",
    "            count = count + 1\n",
    "            continue\n",
    "        # Control line\n",
    "        # if count >= 3:\n",
    "        #     break\n",
    "        # print(row['text'])\n",
    "        # print(pd.isnull(row['text']))\n",
    "        if pd.isnull(row['text']):\n",
    "            continue\n",
    "        doc = nlp(row['text'])\n",
    "        chunck_text = \"\" \n",
    "        chunk_tags = \"\" \n",
    "        pos_tags = \"\"\n",
    "        for i, token in enumerate(doc):\n",
    "            if i!=0:\n",
    "                chunck_text+=\" \"\n",
    "                chunk_tags+=\" \"\n",
    "                pos_tags+=\" \"\n",
    "            chunck_text+=token.text\n",
    "            chunk_tags+=token.dep_\n",
    "            pos_tags+=token.tag_\n",
    "            \n",
    "        f.write(chunck_text)\n",
    "        f.write(\",\")\n",
    "        f.write(pos_tags)\n",
    "        f.write(\",\")\n",
    "        f.write(token.pos_)\n",
    "        f.write(\",\\n\")\n",
    "        count = count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aba61552",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/homeL/1sliu/anaconda3/envs/jarvix_qa/lib/python3.8/site-packages/spacy/util.py:837: UserWarning: [W095] Model 'zh_core_web_lg' (3.5.0) was trained with spaCy v3.5 and may not be 100% compatible with the current version (3.3.2). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "# Collection the index of pos, chunk, and ner\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import json\n",
    "\n",
    "nlp = spacy.load(\"zh_core_web_lg\")\n",
    "\n",
    "data_path = \"data/chunk_question_v1_1.csv\"\n",
    "train_path = \"data/train_new1.json\"\n",
    "valid_path = \"data/valid_new1.json\"\n",
    "test_path = \"data/test_new1.json\"\n",
    "\n",
    "tags = {\"org_text\":0, \"lemma_text\":1, \"pos_tags\":2, \"chunk_tags\":3, \"ner_tags\":4}\n",
    "pos_mapping = {'\"': 0, \"''\": 1, '#': 2, '$': 3, '(': 4, ')': 5, ',': 6, '.': 7, ':': 8, '``': 9, 'NN': 10, 'KON': 11, '$.': 12, 'APPRART': 13, 'ADJA': 14, 'NE': 15, 'VVFIN': 16, 'APPR': 17, '$,': 18, 'CARD': 19, 'TRUNC': 20, 'VAFIN': 21, 'ART': 22, 'VVPP': 23, 'PPER': 24, 'PAV': 25, 'ADJD': 26, 'PTKNEG': 27, 'PTKZU': 28, 'VVINF': 29, 'ADV': 30, 'PRELS': 31, 'PRF': 32, 'PDAT': 33, 'KOUS': 34, 'PIS': 35, 'PIAT': 36, 'VMFIN': 37, 'PPOSAT': 38, 'VVIZU': 39, 'VAPP': 40, '$(': 41, 'VAINF': 42, 'PTKVZ': 43, 'PIDAT': 44, 'KOKOM': 45, 'PDS': 46, 'PTKA': 47, 'PRELAT': 48, 'PWAV': 49, 'PWAT': 50, 'KOUI': 51, 'VMINF': 52, 'PWS': 53, 'APPO': 54, 'VVIMP': 55, 'PTKANT': 56, 'FM': 57, 'APZR': 58, 'VAIMP': 59, 'ITJ': 60, 'PPOSS': 61, 'XY': 62, 'VMPP': 63, 'NR': 64, 'AD': 65, 'CC': 66, 'VC': 67, 'DT': 68, 'DEC': 69, 'DEG': 70, 'P': 71, 'VV': 72, 'CD': 73, 'JJ': 74, 'M': 75, 'OD': 76, 'PU': 77, 'NT': 78, 'LC': 79, 'PN': 80, 'VA': 81, 'VE': 82, '_SP':83}\n",
    "pos_count = 83 \n",
    "chunk_mapping = {'O': 0, 'compound:nn': 1, 'nsubj': 2, 'ROOT': 3, 'dep': 4, 'conj': 5, 'cc': 6, 'cop': 7, 'appos': 8, 'det': 9, 'nmod:assmod': 10, 'case': 11, 'nmod': 12, 'dobj': 13, 'amod': 14, 'advmod': 15, 'mark:clf': 16, 'acl': 17, 'mark': 18, 'nummod': 19, 'nmod:prep': 20, 'nmod:range': 21, 'nmod:tmod': 22, 'amod:ordmod': 23, 'punct': 24, 'ccomp': 25, 'advmod:rcomp': 26, 'advmod:loc': 27, 'name': 28, 'nmod:topic': 29}\n",
    "chunk_count = 29 \n",
    "ner_mapping = {\"O\":0, \"I-Column\": 1, \"B-Column\":2, \"I-Operation\":3, \"B-Operation\":4, \"I-Restriction\":5, \"B-Restriction\":6, \"I-Pattern\":7, \"B-Pattern\":8, \"I-Others\":9, \"B-Others\": 10}\n",
    "ner_count = 9\n",
    "\n",
    "dataframe = pd.read_csv(data_path,sep=',',header=0)\n",
    "dataframe.head()\n",
    "\n",
    "with open(valid_path,'w',encoding='utf-8') as valid:\n",
    "    with open(test_path,'w',encoding='utf-8') as test:\n",
    "        with open(train_path,'w',encoding='utf-8') as train:\n",
    "            for index, row in dataframe.iterrows():\n",
    "                # if index >= 1:\n",
    "                #     break\n",
    "                text = row[\"text\"].replace(' ','')\n",
    "                doc = nlp(text)\n",
    "                \n",
    "                result = {}\n",
    "                result[\"id\"] = str(index)\n",
    "                result[\"chunk_tags\"] = []\n",
    "                result[\"ner_tags\"] = []\n",
    "                result[\"pos_tags\"] = []\n",
    "                result[\"tokens\"] = []\n",
    "\n",
    "                chunk_tags = []\n",
    "                ner_tags = []\n",
    "                pos_tags = []\n",
    "                tokens = []\n",
    "                for i, token in enumerate(doc):\n",
    "                    # print(f\"token.dep_:{token.dep_}\") # chunk tag\n",
    "\n",
    "                    # collect chunk tag\n",
    "                    chunk_tag = token.dep_\n",
    "                    chunk_tags.append(chunk_mapping[chunk_tag])\n",
    "                    \n",
    "                    # # collect unknown chunk tag\n",
    "                    # if chunk_tag not in chunk_mapping:\n",
    "                    #     chunk_mapping[chunk_tag] = chunk_count\n",
    "                    #     chunk_count = chunk_count + 1\n",
    "                    # print(f\"token.tag_:{token.tag_}\") # pos tag\n",
    "\n",
    "                    # collect pos tag\n",
    "                    pos_tag = token.tag_\n",
    "                    pos_tags.append(pos_mapping[pos_tag])\n",
    "\n",
    "                    # # collect unknown pos tag\n",
    "                    # if pos_tag not in pos_mapping:\n",
    "                    #     pos_mapping[pos_tag] = pos_count\n",
    "                    #     pos_count = pos_count + 1\n",
    "\n",
    "                    # collect ner tag\n",
    "                    ner_tags_list = row[\"ner_tags\"].split()\n",
    "                    e_ner_tag = int(ner_tags_list[i])\n",
    "                    # skip column name\n",
    "                    if e_ner_tag == 1 or e_ner_tag == 2:\n",
    "                        ner_tags.append(0)\n",
    "                    else:\n",
    "                        ner_tags.append(e_ner_tag)\n",
    "\n",
    "                    # collect tokens\n",
    "                    # print(token.text)\n",
    "                    tokens.append(token.text)\n",
    "\n",
    "                # Debug\n",
    "                # if index > 176:\n",
    "                #     print(f\"len_ner_tags:{len(ner_tags)}\")\n",
    "                #     print(f\"len_origin_text:{(row['text'].split())}\")\n",
    "                #     print(f\"tokens_origin_text:{(tokens)}\")\n",
    "\n",
    "                result[\"chunk_tags\"] = chunk_tags \n",
    "                result[\"ner_tags\"] = ner_tags \n",
    "                result[\"pos_tags\"] = pos_tags \n",
    "                result[\"tokens\"] = tokens \n",
    "                # print(result)\n",
    "                if index % 7 == 0 or index % 8 ==0:\n",
    "                    json.dump(result,valid)\n",
    "                    valid.write(\"\\n\")\n",
    "                elif index % 9 == 0:\n",
    "                    json.dump(result,test)\n",
    "                    test.write(\"\\n\")\n",
    "                else:\n",
    "                    json.dump(result,train)\n",
    "                    train.write(\"\\n\")\n",
    "                # chunk_tags\n",
    "                # row[\"chunk_tags\"].split()\n",
    "                # print(index)\n",
    "\n",
    "        # print(f\"chunk_mapping:{chunk_mapping}\") \n",
    "        # print(f\"chunk_count:{chunk_count}\") \n",
    "        # print(f\"pos_mapping:{pos_mapping}\") \n",
    "        # print(f\"pos_count:{pos_count}\") \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a78cfb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jarvix_qa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "1b0a29f53631678c0166a40f49938c70010e53514e05336d0e980dc17fe0cd1e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
